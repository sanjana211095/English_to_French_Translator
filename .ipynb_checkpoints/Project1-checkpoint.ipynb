{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec43bdb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b831ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import GRU,Input,Dense,TimeDistributed,Activation,RepeatVector,Bidirectional,Dropout,LSTM,Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47008599",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e63e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_data = \"small_vocab_en.txt\"\n",
    "french_data = \"small_vocab_fr.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f77463d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open (input_file,\"r\") as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0dec5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentence = load_data(english_data)\n",
    "french_sentence = load_data(french_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "62a1c99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mummy,Papa I miss you .\n",
      "Maman, Papa tu me manques .\n",
      "----------------------------------------------------------------------\n",
      "Sakshi I Love You The Most .\n",
      "Sakshi je t'aime le plus .\n",
      "----------------------------------------------------------------------\n",
      "Saurabh I am Proud of You .\n",
      "Saurabh, je suis fier de toi.\n",
      "----------------------------------------------------------------------\n",
      "Shubham I Love You .\n",
      "Shubham je t'aime .\n",
      "----------------------------------------------------------------------\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(english_sentence[i])\n",
    "    print(french_sentence[i])\n",
    "    print('-'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264a7cc",
   "metadata": {},
   "source": [
    "## Convert to Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c000eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a9374e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary:  243\n",
      "French Vocabulary:  372\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentence for word in sentence.split()]) #list Comprehension\n",
    "print(\"English Vocabulary: \",len(english_words_counter))\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentence for word in sentence.split()]) #list Comprehension\n",
    "print(\"French Vocabulary: \",len(french_words_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc213ec",
   "metadata": {},
   "source": [
    "## Tokenize (implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2f513dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x),tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e126bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'short': 4, 'sentence': 5, 'nitin': 6, 'i': 7, 'am': 8, 'lucky': 9, 'to': 10, 'have': 11, 'friend': 12, 'like': 13, 'you': 14}\n",
      "Sequence 1 in x\n",
      "Input: This is a short sentence .Nitin,I am lucky to have friend like you . \n",
      "Input: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] \n"
     ]
    }
   ],
   "source": [
    "text_sentences = [\n",
    "    'This is a short sentence .'\n",
    "    'Nitin,I am lucky to have friend like you .'\n",
    "]\n",
    "text_tokenized,text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "\n",
    "for sample_i,(sent,token_sent) in enumerate (zip(text_sentences,text_tokenized)):\n",
    "    print(\"Sequence {} in x\".format(sample_i+1))\n",
    "    print(\"Input: {} \".format(sent))\n",
    "    print(\"Input: {} \".format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3371e33",
   "metadata": {},
   "source": [
    "## Padding (implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "662f40de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    return pad_sequences(x,maxlen=length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c720ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define process function with x and y\n",
    "def preprocess(x,y):\n",
    "    preprocess_x,x_tk = tokenize(x)\n",
    "    preprocess_y,y_tk = tokenize(y)\n",
    "    \n",
    "    #padding the data\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    #keras's sparese_categorical_crossentropy function requires the labels to be in 3 dimension\n",
    "    #Expanding dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape,1)\n",
    "    return preprocess_x,preprocess_y,x_tk,y_tk\n",
    "\n",
    "#preproc_english_sentence,preproc_french_sentence,english_tokenizer,french_tokenizer \n",
    "preproc_english_sentence,preproc_french_sentence,english_tokenizer,french_tokenizer  =\\\n",
    "preprocess(english_sentence,french_sentence)\n",
    "\n",
    "#print max english sentence length\n",
    "max_english_sentence_length = preproc_english_sentence.shape[1]\n",
    "#print max french sentence length\n",
    "max_french_sentence_length = preproc_french_sentence.shape[1]\n",
    "#print len of englsih vocabulary\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "#print len of englsih vocabulary\n",
    "french_vocab_size = len(french_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd7f88d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English Sentence Length: 15\n",
      "Max French Sentence Length: 21\n",
      "English Voacbulary Size: 211\n",
      "French Voacbulary Size: 361\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Preprocessed\")\n",
    "print(\"Max English Sentence Length:\",max_english_sentence_length)\n",
    "print(\"Max French Sentence Length:\",max_french_sentence_length)\n",
    "print(\"English Voacbulary Size:\",english_vocab_size)\n",
    "print(\"French Voacbulary Size:\",french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91316cff",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b11e7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits,tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "                      \n",
    "    index_to_words[0] = '<PAD>'\n",
    "    \n",
    "    return ''.join([index_to_words[prediction] for prediction in np.argmax(logits,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09466335",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "23c7fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(input_shape,output_sequence_length,english_vocab_size,french_vocab_size):\n",
    "    #hyperparameter\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    #build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size,256,input_length = input_shape[1],input_shape = input_shape[1:]))\n",
    "    \n",
    "    #add GRU layer of 256\n",
    "    model.add(GRU(256,return_sequences=True))\n",
    "    \n",
    "    #add Time distribute layer dense\n",
    "    model.add(TimeDistributed(Dense(1024,activation='relu')))\n",
    "    \n",
    "    #adding dropout layer\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #model Time distributed dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cef6c0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentence,preproc_french_sentence.shape[1])\n",
    "tmp_x = tmp_x.reshape(-1,preproc_french_sentence.shape[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2485125",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentence.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9eb3316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 21, 256)           54272     \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDi  (None, 21, 1024)          263168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 21, 362)           371050    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1083242 (4.13 MB)\n",
      "Trainable params: 1083242 (4.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Print model summary\n",
    "simple_rnn_model.summary()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8e488",
   "metadata": {},
   "source": [
    "## Training the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d36cacb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137865"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "feec37eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137865"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc_french_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b8048378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "108/108 [==============================] - 124s 1s/step - loss: 1.3589 - accuracy: 0.6822 - val_loss: 0.4783 - val_accuracy: 0.8464\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 125s 1s/step - loss: 0.4031 - accuracy: 0.8681 - val_loss: 0.2983 - val_accuracy: 0.9001\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 123s 1s/step - loss: 0.2902 - accuracy: 0.9032 - val_loss: 0.2467 - val_accuracy: 0.9153\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.2428 - accuracy: 0.9177 - val_loss: 0.2191 - val_accuracy: 0.9249\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.2188 - accuracy: 0.9251 - val_loss: 0.1998 - val_accuracy: 0.9305\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.2042 - accuracy: 0.9291 - val_loss: 0.1952 - val_accuracy: 0.9319\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 123s 1s/step - loss: 0.1944 - accuracy: 0.9319 - val_loss: 0.1899 - val_accuracy: 0.9335\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 122s 1s/step - loss: 0.1893 - accuracy: 0.9333 - val_loss: 0.1825 - val_accuracy: 0.9353\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 120s 1s/step - loss: 0.1845 - accuracy: 0.9343 - val_loss: 0.1844 - val_accuracy: 0.9354\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1798 - accuracy: 0.9359 - val_loss: 0.1792 - val_accuracy: 0.9373\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.1764 - accuracy: 0.9367 - val_loss: 0.1807 - val_accuracy: 0.9363\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1727 - accuracy: 0.9378 - val_loss: 0.1776 - val_accuracy: 0.9372\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1700 - accuracy: 0.9384 - val_loss: 0.1783 - val_accuracy: 0.9372\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1686 - accuracy: 0.9387 - val_loss: 0.1786 - val_accuracy: 0.9372\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1711 - accuracy: 0.9381 - val_loss: 0.1852 - val_accuracy: 0.9368\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 124s 1s/step - loss: 0.1669 - accuracy: 0.9391 - val_loss: 0.1764 - val_accuracy: 0.9391\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 121s 1s/step - loss: 0.1643 - accuracy: 0.9399 - val_loss: 0.1781 - val_accuracy: 0.9382\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1628 - accuracy: 0.9402 - val_loss: 0.1780 - val_accuracy: 0.9389\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1623 - accuracy: 0.9403 - val_loss: 0.1827 - val_accuracy: 0.9381\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.1726 - accuracy: 0.9378 - val_loss: 0.1863 - val_accuracy: 0.9371\n"
     ]
    }
   ],
   "source": [
    "history = simple_rnn_model.fit(tmp_x,preproc_french_sentence,batch_size=1024,\n",
    "                              epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9952dba",
   "metadata": {},
   "source": [
    "## Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "11a27998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model.save(\"Minimodel.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d73bf6",
   "metadata": {},
   "source": [
    "## Arbitary Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "905a3866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1,\n",
       " 'in': 2,\n",
       " 'it': 3,\n",
       " 'during': 4,\n",
       " 'the': 5,\n",
       " 'but': 6,\n",
       " 'and': 7,\n",
       " 'sometimes': 8,\n",
       " 'usually': 9,\n",
       " 'never': 10,\n",
       " 'favorite': 11,\n",
       " 'least': 12,\n",
       " 'fruit': 13,\n",
       " 'most': 14,\n",
       " 'loved': 15,\n",
       " 'liked': 16,\n",
       " 'new': 17,\n",
       " 'paris': 18,\n",
       " 'india': 19,\n",
       " 'united': 20,\n",
       " 'states': 21,\n",
       " 'california': 22,\n",
       " 'jersey': 23,\n",
       " 'france': 24,\n",
       " 'china': 25,\n",
       " 'he': 26,\n",
       " 'she': 27,\n",
       " 'grapefruit': 28,\n",
       " 'your': 29,\n",
       " 'my': 30,\n",
       " 'his': 31,\n",
       " 'her': 32,\n",
       " 'fall': 33,\n",
       " 'june': 34,\n",
       " 'spring': 35,\n",
       " 'january': 36,\n",
       " 'winter': 37,\n",
       " 'march': 38,\n",
       " 'autumn': 39,\n",
       " 'may': 40,\n",
       " 'nice': 41,\n",
       " 'september': 42,\n",
       " 'july': 43,\n",
       " 'april': 44,\n",
       " 'november': 45,\n",
       " 'summer': 46,\n",
       " 'december': 47,\n",
       " 'february': 48,\n",
       " 'our': 49,\n",
       " 'their': 50,\n",
       " 'freezing': 51,\n",
       " 'pleasant': 52,\n",
       " 'beautiful': 53,\n",
       " 'october': 54,\n",
       " 'snowy': 55,\n",
       " 'warm': 56,\n",
       " 'cold': 57,\n",
       " 'wonderful': 58,\n",
       " 'dry': 59,\n",
       " 'busy': 60,\n",
       " 'august': 61,\n",
       " 'chilly': 62,\n",
       " 'rainy': 63,\n",
       " 'mild': 64,\n",
       " 'wet': 65,\n",
       " 'relaxing': 66,\n",
       " 'quiet': 67,\n",
       " 'hot': 68,\n",
       " 'dislikes': 69,\n",
       " 'likes': 70,\n",
       " 'limes': 71,\n",
       " 'lemons': 72,\n",
       " 'grapes': 73,\n",
       " 'mangoes': 74,\n",
       " 'apples': 75,\n",
       " 'peaches': 76,\n",
       " 'oranges': 77,\n",
       " 'pears': 78,\n",
       " 'strawberries': 79,\n",
       " 'bananas': 80,\n",
       " 'to': 81,\n",
       " 'grape': 82,\n",
       " 'apple': 83,\n",
       " 'orange': 84,\n",
       " 'lemon': 85,\n",
       " 'lime': 86,\n",
       " 'banana': 87,\n",
       " 'mango': 88,\n",
       " 'pear': 89,\n",
       " 'strawberry': 90,\n",
       " 'peach': 91,\n",
       " 'like': 92,\n",
       " 'dislike': 93,\n",
       " 'they': 94,\n",
       " 'that': 95,\n",
       " 'i': 96,\n",
       " 'we': 97,\n",
       " 'you': 98,\n",
       " 'animal': 99,\n",
       " 'a': 100,\n",
       " 'truck': 101,\n",
       " 'car': 102,\n",
       " 'automobile': 103,\n",
       " 'was': 104,\n",
       " 'next': 105,\n",
       " 'go': 106,\n",
       " 'driving': 107,\n",
       " 'visit': 108,\n",
       " 'little': 109,\n",
       " 'big': 110,\n",
       " 'old': 111,\n",
       " 'yellow': 112,\n",
       " 'red': 113,\n",
       " 'rusty': 114,\n",
       " 'blue': 115,\n",
       " 'white': 116,\n",
       " 'black': 117,\n",
       " 'green': 118,\n",
       " 'shiny': 119,\n",
       " 'are': 120,\n",
       " 'last': 121,\n",
       " 'feared': 122,\n",
       " 'animals': 123,\n",
       " 'this': 124,\n",
       " 'plan': 125,\n",
       " 'going': 126,\n",
       " 'saw': 127,\n",
       " 'disliked': 128,\n",
       " 'drives': 129,\n",
       " 'drove': 130,\n",
       " 'between': 131,\n",
       " 'translate': 132,\n",
       " 'plans': 133,\n",
       " 'were': 134,\n",
       " 'went': 135,\n",
       " 'might': 136,\n",
       " 'wanted': 137,\n",
       " 'thinks': 138,\n",
       " 'spanish': 139,\n",
       " 'portuguese': 140,\n",
       " 'chinese': 141,\n",
       " 'english': 142,\n",
       " 'french': 143,\n",
       " 'translating': 144,\n",
       " 'difficult': 145,\n",
       " 'fun': 146,\n",
       " 'easy': 147,\n",
       " 'wants': 148,\n",
       " 'think': 149,\n",
       " 'why': 150,\n",
       " \"it's\": 151,\n",
       " 'did': 152,\n",
       " 'cat': 153,\n",
       " 'shark': 154,\n",
       " 'bird': 155,\n",
       " 'mouse': 156,\n",
       " 'horse': 157,\n",
       " 'elephant': 158,\n",
       " 'dog': 159,\n",
       " 'monkey': 160,\n",
       " 'lion': 161,\n",
       " 'bear': 162,\n",
       " 'rabbit': 163,\n",
       " 'snake': 164,\n",
       " 'when': 165,\n",
       " 'want': 166,\n",
       " 'do': 167,\n",
       " 'how': 168,\n",
       " 'elephants': 169,\n",
       " 'horses': 170,\n",
       " 'dogs': 171,\n",
       " 'sharks': 172,\n",
       " 'snakes': 173,\n",
       " 'cats': 174,\n",
       " 'rabbits': 175,\n",
       " 'monkeys': 176,\n",
       " 'bears': 177,\n",
       " 'birds': 178,\n",
       " 'lions': 179,\n",
       " 'mice': 180,\n",
       " \"didn't\": 181,\n",
       " 'eiffel': 182,\n",
       " 'tower': 183,\n",
       " 'grocery': 184,\n",
       " 'store': 185,\n",
       " 'football': 186,\n",
       " 'field': 187,\n",
       " 'lake': 188,\n",
       " 'school': 189,\n",
       " 'would': 190,\n",
       " \"aren't\": 191,\n",
       " 'been': 192,\n",
       " 'weather': 193,\n",
       " 'am': 194,\n",
       " 'does': 195,\n",
       " 'has': 196,\n",
       " \"isn't\": 197,\n",
       " 'have': 198,\n",
       " 'where': 199,\n",
       " 'love': 200,\n",
       " 'mummy': 201,\n",
       " 'papa': 202,\n",
       " 'miss': 203,\n",
       " 'sakshi': 204,\n",
       " 'saurabh': 205,\n",
       " 'proud': 206,\n",
       " 'of': 207,\n",
       " 'shubham': 208,\n",
       " 'nitin': 209,\n",
       " 'lucky': 210,\n",
       " 'friend': 211}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b68169fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def final_predictions(text):\n",
    "#     y_id_to_word = {value: key for key,value in french_tokenizer.word_index.items()}\n",
    "#     y_id_to_word[0] = '<PAD>'\n",
    "#     sentence = [english_tokenizer.word_index[word] for word in text.split()]\n",
    "#     sentence = pad_sequences([sentence],maxlen=preproc_french_sentence.shape[-2],padding='post')\n",
    "# #     print(sentence)\n",
    "#     text1 = logits_to_text(simple_rnn_model.predict(sentence[:1])[0],french_tokenizer)\n",
    "#     text2 = \"\"\n",
    "#     #iterate over text\n",
    "#     for i in text1.split():\n",
    "#         if i == '<PAD>':\n",
    "#             break\n",
    "#         else:\n",
    "#             print(i)\n",
    "#             text2 = text2+\" \"+i\n",
    "#     return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4b497ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predictions(text):\n",
    "    sentence = [english_tokenizer.word_index.get(word, 0) for word in text.split()]  # Replace unknown words with 0\n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_french_sentence.shape[-2], padding='post')\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = simple_rnn_model.predict(sentence[:1])[0]\n",
    "    \n",
    "    # Convert predictions to text\n",
    "    text2 = logits_to_text(predictions, french_tokenizer)\n",
    "    \n",
    "    # Remove padding from the generated text\n",
    "    text2 = text2.replace('<PAD>', '').strip()\n",
    "    \n",
    "    return text2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a68a1027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is during the spring\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ilestenlesprintemps'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3d273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
